{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,Conv1D, Dense, MaxPool1D, Activation, AvgPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import Flatten, Add, Concatenate, Dropout, BatchNormalization, merge\n",
    "from keras import regularizers, initializers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_CNN = True\n",
    "activation = 'relu' # selu\n",
    "use_meta = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''read signal'''\n",
    "file_path = '/data/put_data/timliu/BG/'\n",
    "if multi_CNN:\n",
    "    with open(file_path+'train_test/data/0918/dict_X_C10_S5_50.0.pickle', 'rb') as handle:\n",
    "        X50 = pickle.load(handle)\n",
    "    with open(file_path+'train_test/data/0918/dict_X_C10_S5_100.0.pickle', 'rb') as handle: # 100\n",
    "        X5 = pickle.load(handle) \n",
    "    with open(file_path+'train_test/data/0918/dict_X_C10_S5_200.0.pickle', 'rb') as handle: \n",
    "        X10 = pickle.load(handle) \n",
    "    with open(file_path+'train_test/data/0918/dict_X_C10_S5_25.0.pickle', 'rb') as handle: \n",
    "        X25 = pickle.load(handle) \n",
    "else:\n",
    "    with open('train_test/data/0918/dict_X_C10_S5_200.0.pickle', 'rb') as handle:\n",
    "        X = pickle.load(handle)\n",
    "'''read meta'''\n",
    "with open(file_path+'train_test/dict_meta3_0901.pickle', 'rb') as handle:\n",
    "    meta = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Person No Age Gender Height  SYS  DIA   HR    G  PWV   BMI  ...      f18  \\\n",
      "0         3  53      0    157  143  104   73  106  6.5  25.4  ...    0.001   \n",
      "1         3  53      0    157  144   95   73   95  6.5  25.4  ...   0.0013   \n",
      "2         4  49      0    150  117   85   75   99    7  23.4  ...    8e-04   \n",
      "3         4  49      0    150  133   92   80   94    7  23.4  ...    0.001   \n",
      "4         5  55      1    167  142   79  104   88    7  24.4  ...   0.0014   \n",
      "\n",
      "      f19     f20     f21     f22     f23          f24     ID      Date  Time  \n",
      "0  353268  0.8983  0.0049  0.0017  131210  0.810468912  003_1  20170302  0935  \n",
      "1  356900  0.9129  0.0053  0.0027  119821  0.820373018  003_2  20170302  0939  \n",
      "2  657728  0.8002  0.0024  0.0011  240208  0.741961393  004_1  20170303  0817  \n",
      "3  447514  0.7975   0.003  0.0025  156888  0.721089641  004_2  20170303  0827  \n",
      "4  380280  0.5866  0.0015  0.0021  130575  0.558661023  005_1  20170303  0837  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# data split\n",
    "split_ratio = 0.8\n",
    "# model parameter\n",
    "l_2 = 1e-6\n",
    "dropout_rate = 0.4\n",
    "lr_rate = 0.001\n",
    "\n",
    "# Embedding\n",
    "n_sample = 6000\n",
    "n_meta = len(meta['003_1'])\n",
    "n_channel = 10\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 15\n",
    "local_filters = 16\n",
    "local_kernel_size = 3\n",
    "local_pool_size = 2\n",
    "filters = 64\n",
    "filter_size = 15\n",
    "strides = 1\n",
    "depth = 29\n",
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "n_batch_per_epoch = 200\n",
    "epochs = 50\n",
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "d = pd.read_csv(file_path + \"train_test/data/new_IRB_summary_804.5.csv\",dtype=\"str\")\n",
    "print(d[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''split training and testing data set'''\n",
    "D_id = list(set([row['Person No'] for index, row in d.iterrows() if float(row['G']) >= 200]))\n",
    "np.random.shuffle(D_id)\n",
    "ND_id = np.asarray(d.loc[np.asarray(d['G'], dtype = 'float64') < 200]['Person No'].unique())\n",
    "np.random.shuffle(ND_id)\n",
    "\n",
    "train_id = np.hstack((ND_id[:int(len(ND_id) * split_ratio)], D_id[:int(len(D_id) * split_ratio)]))\n",
    "# train_id = np.hstack((ND_id[:int(len(ND_id) * split_ratio)]))\n",
    "train_id\n",
    "\n",
    "test_id = np.hstack((ND_id[int(len(ND_id) * split_ratio):], D_id[int(len(D_id) * split_ratio):]))\n",
    "# test_id = np.hstack((ND_id[int(len(ND_id) * split_ratio):]))\n",
    "test_id\n",
    "\n",
    "test_ind = [index for index, row in d.iterrows() if np.any(test_id == row['Person No'])] \n",
    "train_ind = [index for index, row in d.iterrows() if np.any(train_id == row['Person No'])]   \n",
    "\n",
    "train_file = np.array(d['ID'][train_ind])\n",
    "test_file = np.array(d['ID'][test_ind])\n",
    "lab = np.array(d['G']).astype('float32')\n",
    "train_lab = np.array(lab[train_ind])\n",
    "train_lab = train_lab.astype('float32')\n",
    "test_lab  = np.array(lab[test_ind])\n",
    "test_lab  = test_lab.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_generator(batch_size):\n",
    "    if multi_CNN:\n",
    "        while 1:\n",
    "            X5_ = []\n",
    "            X10_ = []\n",
    "            X25_ = []\n",
    "            X50_ = []\n",
    "            M_ = []\n",
    "            Y_ = []\n",
    "            for _ in range(batch_size):\n",
    "                # randomly select a record\n",
    "                i = random.randint(0,len(train_file)-1)\n",
    "                x = X5[train_file[i]] \n",
    "                x10 = X10[train_file[i]]\n",
    "                x25 = X25[train_file[i]]\n",
    "                x50 = X50[train_file[i]]\n",
    "                x2 = meta[train_file[i]]\n",
    "                # random select a subrecord from 0~4\n",
    "                j = random.randint(0, 4)\n",
    "                x1 = x[j]\n",
    "                x101 = x10[j]\n",
    "                x251 = x25[j]\n",
    "                x501 = x50[j]\n",
    "                # append on list\n",
    "                X5_.append(x1)\n",
    "                X10_.append(x101)\n",
    "                X25_.append(x251)\n",
    "                X50_.append(x501)\n",
    "                M_.append(x2)\n",
    "                Y_.append(train_lab[i])\n",
    "            # stack together batch size numbers of training data    \n",
    "            X5_ = np.asarray(X5_)\n",
    "            X10_ = np.asarray(X10_)\n",
    "            X25_ = np.asarray(X25_)\n",
    "            X50_ = np.asarray(X50_)\n",
    "            M_ = np.asarray(M_)\n",
    "            if use_meta:\n",
    "                COM_ = [X5_,X10_,X25_,X50_,M_]\n",
    "            else:\n",
    "                COM_ = [X5_,X10_,X25_,X50_]\n",
    "            Y_ = np.asarray(Y_)\n",
    "            yield COM_, Y_\n",
    "    else:    \n",
    "        while 1:\n",
    "            X_ = []\n",
    "            M_ = []\n",
    "            Y_ = []\n",
    "            for _ in batch_size:\n",
    "                id_ = train_file[random.randint(0, len(train_file) - 1)] # random select a sample\n",
    "                signal_id = random.randint(0, 4) # random select a signal\n",
    "                x = X[id_][signal_id]\n",
    "                X_.append(x)\n",
    "                M_.append(meta[id_])\n",
    "                Y_.append(train_lab[id_])\n",
    "            X_ = np.asarray(X_)\n",
    "            M_ = np.asarray(M_)\n",
    "            Y_ = np.asarray(Y_)\n",
    "            if use_meta:\n",
    "                yield [X_, M_], Y_\n",
    "            else:\n",
    "                yield [X_], Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_generation(test_file,test_lab):\n",
    "    if multi_CNN:\n",
    "        X5_ = []\n",
    "        X10_ = []\n",
    "        X25_ = []\n",
    "        X50_ = []\n",
    "        M_ = []\n",
    "        Y_ = []\n",
    "        for i in range(len(test_file)):\n",
    "            x = X5[test_file[i]] \n",
    "            x10 = X10[test_file[i]]\n",
    "            x25 = X25[test_file[i]]\n",
    "            x50 = X50[test_file[i]]\n",
    "            x2 = meta[test_file[i]]\n",
    "            for j in range(5):\n",
    "                X5_.append(x[j])\n",
    "                X10_.append(x10[j])\n",
    "                X25_.append(x25[j])\n",
    "                X50_.append(x50[j])\n",
    "                M_.append(x2)\n",
    "                Y_.append(test_lab[i])\n",
    "        X5_ = np.asarray(X5_)\n",
    "        X10_ = np.asarray(X10_)\n",
    "        X25_ = np.asarray(X25_)\n",
    "        X50_ = np.asarray(X50_)\n",
    "        M_ = np.asarray(M_)\n",
    "    #     COM_ = [X5_,X10_,X25_,X50_,M_]\n",
    "        Y_ = np.asarray(Y_)\n",
    "        print(\"# records = \", len(Y_))\n",
    "        if use_meta:\n",
    "            return X5_, X10_,X25_,X50_,M_, Y_ \n",
    "        else:\n",
    "            return X5_, X10_,X25_,X50_, Y_\n",
    "    else:\n",
    "        X_ = []\n",
    "        M_ = []\n",
    "        Y_ = []\n",
    "        for i in range(len(test_file)):\n",
    "            id_ = test_file[i]\n",
    "            for j in range(5):\n",
    "                signal_id = j # random select a signal\n",
    "                x = X[id_][signal_id]\n",
    "                X_.append(x)\n",
    "                M_.append(meta[id_])\n",
    "                Y_.append(test_lab[id_])\n",
    "        X_ = np.asarray(X_)\n",
    "        M_ = np.asarray(M_)\n",
    "        Y_ = np.asarray(Y_)\n",
    "        print(\"# records = \", len(Y_))\n",
    "        if use_meta:\n",
    "            return X_, M_, Y_\n",
    "        else:\n",
    "            return X_, Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_train_set(size = 1610):\n",
    "    if multi_CNN:\n",
    "        X5_ = []\n",
    "        X10_ = []\n",
    "        X25_ = []\n",
    "        X50_ = []\n",
    "        M_ = []\n",
    "        Y_ = []\n",
    "        count = 0\n",
    "        while count < size:\n",
    "            count = count + 1\n",
    "            # select a random sample\n",
    "            i = random.randint(0,len(train_file)-1)\n",
    "            x = X5[train_file[i]] \n",
    "            x10 = X10[train_file[i]]\n",
    "            x25 = X25[train_file[i]]\n",
    "            x50 = X50[train_file[i]]\n",
    "            x2 = meta[train_file[i]]\n",
    "            j = random.randint(0, 4)\n",
    "            x1 = x[j]\n",
    "            x101 = x10[j]\n",
    "            x251 = x25[j]\n",
    "            x501 = x50[j]\n",
    "            X5_.append(x1)\n",
    "            X10_.append(x101)\n",
    "            X25_.append(x251)\n",
    "            X50_.append(x501)\n",
    "            M_.append(x2)\n",
    "            Y_.append(train_lab[i])\n",
    "        X5_ = np.asarray(X5_)\n",
    "        X10_ = np.asarray(X10_)\n",
    "        X25_ = np.asarray(X25_)\n",
    "        X50_ = np.asarray(X50_)\n",
    "    #     COM_ = [X5_,X10_,X25_,X50_,M_]\n",
    "        M_ = np.asarray(M_)\n",
    "        Y_ = np.asarray(Y_) \n",
    "        if use_meta:\n",
    "            return X5_, X10_,X25_,X50_,M_, Y_\n",
    "        else:\n",
    "            return X5_, X10_,X25_,X50_, Y_\n",
    "    else:\n",
    "        X_ = []\n",
    "        M_ = []\n",
    "        Y_ = []\n",
    "        count = 0\n",
    "        while count < size:\n",
    "            id_ = train_file[i]\n",
    "            signal_id = random.randint(0,len(train_file)-1) # random select a signal\n",
    "            x = X[id_][signal_id]\n",
    "            X_.append(x)\n",
    "            M_.append(meta[id_])\n",
    "            Y_.append(train_lab[id_])\n",
    "            count += 1\n",
    "        X_ = np.asarray(X_)\n",
    "        M_ = np.asarray(M_)\n",
    "        Y_ = np.asarray(Y_)\n",
    "        if use_meta:\n",
    "            return X_, M_, Y_\n",
    "        else:\n",
    "            return X_,Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_cnn(filters,kernel_size,strides,pool_size,inputs):\n",
    "    if activation == 'selu':\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inputs)\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inp)\n",
    "        inp = MaxPool1D(pool_size = local_pool_size, padding = 'same')(inp)\n",
    "        inp = Activation(activation='selu')(inp)\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inp)\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inp)\n",
    "    else:\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inputs)\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inp)\n",
    "        inp = MaxPool1D(pool_size = local_pool_size, padding = 'same')(inp)\n",
    "        inp = BatchNormalization()(inp)\n",
    "        inp = Activation(activation=activation)(inp)\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inp)\n",
    "        inp = Conv1D(filters = local_filters, kernel_size = local_kernel_size, strides = strides, padding = 'same',kernel_regularizer=regularizers.l2(l_2))(inp)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(ip, nb_filter, filter_size, dropout_rate = None):\n",
    "    x = BatchNormalization()(ip)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv1D(nb_filter, filter_size, strides = 1, kernel_initializer = kernel_initializer, padding = \"same\",kernel_regularizer=regularizers.l2(l_2))(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "def transition_block(ip, nb_filter, filter_size, dropout_rate=None):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "    \n",
    "    x = BatchNormalization()(ip)\n",
    "    x = Conv1D(nb_filter, filter_size, strides = 1, kernel_initializer = kernel_initializer, padding = \"same\",kernel_regularizer=regularizers.l2(l_2))(x)\n",
    "    if dropout_rate:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    x = AvgPool1D(pool_size = 2, strides = 2)(x)\n",
    "    return x\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None):\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    feature_list = [x]\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, filter_size, dropout_rate)\n",
    "        feature_list.append(x)\n",
    "        x = merge(feature_list, mode='concat', concat_axis=concat_axis)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter\n",
    "\n",
    "def create_dense_net(depth=13, nb_dense_block=3, growth_rate=12, nb_filter=32, dropout_rate=None,verbose=True):\n",
    "    sig_inp5 = Input(shape = (3000, 10))\n",
    "    inp5 = local_cnn(local_filters,local_kernel_size,strides,local_pool_size,sig_inp5)\n",
    "    \n",
    "    sig_inp10 = Input(shape = (6000, 10))\n",
    "    inp10 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp10)\n",
    "\n",
    "    sig_inp25 = Input(shape = (750, 10))\n",
    "    inp25 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp25)\n",
    "\n",
    "    sig_inp50 = Input(shape = (1500, 10))\n",
    "    inp50 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp50)\n",
    "    \n",
    "    inp_con = keras.layers.concatenate([inp5, inp10, inp25, inp50], axis = 1)\n",
    "    metai = Input(shape = (n_meta, ))\n",
    "    concat_axis = 1 if K.image_dim_ordering() == \"th\" else -1\n",
    "\n",
    "    assert (depth - (nb_dense_block+1)) % nb_dense_block == 0, \"Depth must be 3 N + 4\"\n",
    "\n",
    "    # layers in each dense block\n",
    "    nb_layers = int((depth - (nb_dense_block+1)) / nb_dense_block)\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Conv1D(nb_filter, filter_size, strides = 1, kernel_initializer=kernel_initializer, padding=\"same\", name=\"initial_conv2D\",kernel_regularizer=regularizers.l2(l_2))(inp_con)\n",
    "\n",
    "    # Add dense blocks\n",
    "    for block_idx in range(nb_dense_block - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "        # add transition_block\n",
    "        x = transition_block(x, nb_filter, filter_size, dropout_rate=dropout_rate)\n",
    "\n",
    "    # The last dense_block does not have a transition_block\n",
    "    x, nb_filter = dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(activation = 'relu')(x)\n",
    "#     x = keras.layers.concatenate([x, metai])\n",
    "#     x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = Dense(64, activation='relu',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = Dense(64, activation='relu',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     x = Dense(1, activation='linear',kernel_regularizer=regularizers.l2(l_2),kernel_initializer=kernel_initializer)(x)\n",
    "    \n",
    "    new_inp = keras.layers.concatenate([x,metai])\n",
    "    new_inp2 = x\n",
    "    new_inp = BatchNormalization()(new_inp)\n",
    "    new_inp = Activation(activation=activation)(new_inp)\n",
    "    #             if use_meta:\n",
    "    #                 new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "    new_inp = Dense(128,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "    new_inp = Dropout(dropout_rate)(new_inp)\n",
    "    new_inp = keras.layers.concatenate([new_inp, new_inp2])\n",
    "    new_inp = BatchNormalization()(new_inp)\n",
    "    new_inp = Activation(activation=activation)(new_inp)\n",
    "    new_inp = Dense(64,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "    new_inp = Dropout(dropout_rate)(new_inp)\n",
    "    out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "\n",
    "    densenet = Model(inputs = [sig_inp5, sig_inp10, sig_inp25, sig_inp50, metai], output = out, name = \"create_dense_net\")\n",
    "\n",
    "    if verbose: print(\"DenseNet-%d-%d created.\" % (depth, growth_rate))\n",
    "\n",
    "    return densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:27: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet-29-12 created.\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 3000, 10)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 6000, 10)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 750, 10)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 1500, 10)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 3000, 16)      496         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 6000, 16)      496         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 750, 16)       496         input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)               (None, 1500, 16)      496         input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 3000, 16)      784         conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 6000, 16)      784         conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)               (None, 750, 16)       784         conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)               (None, 1500, 16)      784         conv1d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 1500, 16)      0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 3000, 16)      0           conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 375, 16)       0           conv1d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 750, 16)       0           conv1d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 1500, 16)      64          max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 3000, 16)      64          max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 375, 16)       64          max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 750, 16)       64          max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1500, 16)      0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 3000, 16)      0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 375, 16)       0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 750, 16)       0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 1500, 16)      784         activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 3000, 16)      784         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)               (None, 375, 16)       784         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)               (None, 750, 16)       784         activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 1500, 16)      784         conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 3000, 16)      784         conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 375, 16)       784         conv1d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)               (None, 750, 16)       784         conv1d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 5625, 16)      0           conv1d_4[0][0]                   \n",
      "                                                                   conv1d_8[0][0]                   \n",
      "                                                                   conv1d_12[0][0]                  \n",
      "                                                                   conv1d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "initial_conv2D (Conv1D)          (None, 5625, 32)      7712        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 5625, 32)      128         initial_conv2D[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 5625, 32)      0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)               (None, 5625, 12)      5772        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 5625, 12)      0           conv1d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 5625, 44)      0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 5625, 44)      176         merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 5625, 44)      0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)               (None, 5625, 12)      7932        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 5625, 12)      0           conv1d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 5625, 56)      0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 5625, 56)      224         merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 5625, 56)      0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)               (None, 5625, 12)      10092       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 5625, 12)      0           conv1d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 5625, 68)      0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 5625, 68)      272         merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 5625, 68)      0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)               (None, 5625, 12)      12252       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 5625, 12)      0           conv1d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 5625, 80)      0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "                                                                   dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 5625, 80)      320         merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 5625, 80)      0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)               (None, 5625, 12)      14412       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 5625, 12)      0           conv1d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_5 (Merge)                  (None, 5625, 92)      0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "                                                                   dropout_4[0][0]                  \n",
      "                                                                   dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 5625, 92)      368         merge_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 5625, 92)      0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)               (None, 5625, 12)      16572       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 5625, 12)      0           conv1d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_6 (Merge)                  (None, 5625, 104)     0           initial_conv2D[0][0]             \n",
      "                                                                   dropout_1[0][0]                  \n",
      "                                                                   dropout_2[0][0]                  \n",
      "                                                                   dropout_3[0][0]                  \n",
      "                                                                   dropout_4[0][0]                  \n",
      "                                                                   dropout_5[0][0]                  \n",
      "                                                                   dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 5625, 104)     416         merge_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)               (None, 5625, 104)     162344      batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 5625, 104)     0           conv1d_23[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePool (None, 2812, 104)     0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 2812, 104)     416         average_pooling1d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 2812, 104)     0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)               (None, 2812, 12)      18732       activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 2812, 12)      0           conv1d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_7 (Merge)                  (None, 2812, 116)     0           average_pooling1d_1[0][0]        \n",
      "                                                                   dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 2812, 116)     464         merge_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 2812, 116)     0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)               (None, 2812, 12)      20892       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 2812, 12)      0           conv1d_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_8 (Merge)                  (None, 2812, 128)     0           average_pooling1d_1[0][0]        \n",
      "                                                                   dropout_8[0][0]                  \n",
      "                                                                   dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 2812, 128)     512         merge_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 2812, 128)     0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)               (None, 2812, 12)      23052       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 2812, 12)      0           conv1d_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_9 (Merge)                  (None, 2812, 140)     0           average_pooling1d_1[0][0]        \n",
      "                                                                   dropout_8[0][0]                  \n",
      "                                                                   dropout_9[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 2812, 140)     560         merge_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 2812, 140)     0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)               (None, 2812, 12)      25212       activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 2812, 12)      0           conv1d_27[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_10 (Merge)                 (None, 2812, 152)     0           average_pooling1d_1[0][0]        \n",
      "                                                                   dropout_8[0][0]                  \n",
      "                                                                   dropout_9[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "                                                                   dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 2812, 152)     608         merge_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 2812, 152)     0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)               (None, 2812, 12)      27372       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 2812, 12)      0           conv1d_28[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_11 (Merge)                 (None, 2812, 164)     0           average_pooling1d_1[0][0]        \n",
      "                                                                   dropout_8[0][0]                  \n",
      "                                                                   dropout_9[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "                                                                   dropout_11[0][0]                 \n",
      "                                                                   dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 2812, 164)     656         merge_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 2812, 164)     0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)               (None, 2812, 12)      29532       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 2812, 12)      0           conv1d_29[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_12 (Merge)                 (None, 2812, 176)     0           average_pooling1d_1[0][0]        \n",
      "                                                                   dropout_8[0][0]                  \n",
      "                                                                   dropout_9[0][0]                  \n",
      "                                                                   dropout_10[0][0]                 \n",
      "                                                                   dropout_11[0][0]                 \n",
      "                                                                   dropout_12[0][0]                 \n",
      "                                                                   dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 2812, 176)     704         merge_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)               (None, 2812, 176)     464816      batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 2812, 176)     0           conv1d_30[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_2 (AveragePool (None, 1406, 176)     0           dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 1406, 176)     704         average_pooling1d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 1406, 176)     0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)               (None, 1406, 12)      31692       activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 1406, 12)      0           conv1d_31[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_13 (Merge)                 (None, 1406, 188)     0           average_pooling1d_2[0][0]        \n",
      "                                                                   dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, 1406, 188)     752         merge_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 1406, 188)     0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)               (None, 1406, 12)      33852       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 1406, 12)      0           conv1d_32[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_14 (Merge)                 (None, 1406, 200)     0           average_pooling1d_2[0][0]        \n",
      "                                                                   dropout_15[0][0]                 \n",
      "                                                                   dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, 1406, 200)     800         merge_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 1406, 200)     0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)               (None, 1406, 12)      36012       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 1406, 12)      0           conv1d_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_15 (Merge)                 (None, 1406, 212)     0           average_pooling1d_2[0][0]        \n",
      "                                                                   dropout_15[0][0]                 \n",
      "                                                                   dropout_16[0][0]                 \n",
      "                                                                   dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, 1406, 212)     848         merge_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 1406, 212)     0           batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)               (None, 1406, 12)      38172       activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 1406, 12)      0           conv1d_34[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_16 (Merge)                 (None, 1406, 224)     0           average_pooling1d_2[0][0]        \n",
      "                                                                   dropout_15[0][0]                 \n",
      "                                                                   dropout_16[0][0]                 \n",
      "                                                                   dropout_17[0][0]                 \n",
      "                                                                   dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNor (None, 1406, 224)     896         merge_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 1406, 224)     0           batch_normalization_23[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)               (None, 1406, 12)      40332       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 1406, 12)      0           conv1d_35[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_17 (Merge)                 (None, 1406, 236)     0           average_pooling1d_2[0][0]        \n",
      "                                                                   dropout_15[0][0]                 \n",
      "                                                                   dropout_16[0][0]                 \n",
      "                                                                   dropout_17[0][0]                 \n",
      "                                                                   dropout_18[0][0]                 \n",
      "                                                                   dropout_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNor (None, 1406, 236)     944         merge_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 1406, 236)     0           batch_normalization_24[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)               (None, 1406, 12)      42492       activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 1406, 12)      0           conv1d_36[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_18 (Merge)                 (None, 1406, 248)     0           average_pooling1d_2[0][0]        \n",
      "                                                                   dropout_15[0][0]                 \n",
      "                                                                   dropout_16[0][0]                 \n",
      "                                                                   dropout_17[0][0]                 \n",
      "                                                                   dropout_18[0][0]                 \n",
      "                                                                   dropout_19[0][0]                 \n",
      "                                                                   dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNor (None, 1406, 248)     992         merge_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)               (None, 1406, 248)     922808      batch_normalization_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 1406, 248)     0           conv1d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling1d_3 (AveragePool (None, 703, 248)      0           dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNor (None, 703, 248)      992         average_pooling1d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 703, 248)      0           batch_normalization_26[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)               (None, 703, 12)       44652       activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 703, 12)       0           conv1d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_19 (Merge)                 (None, 703, 260)      0           average_pooling1d_3[0][0]        \n",
      "                                                                   dropout_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNor (None, 703, 260)      1040        merge_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 703, 260)      0           batch_normalization_27[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)               (None, 703, 12)       46812       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)             (None, 703, 12)       0           conv1d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_20 (Merge)                 (None, 703, 272)      0           average_pooling1d_3[0][0]        \n",
      "                                                                   dropout_22[0][0]                 \n",
      "                                                                   dropout_23[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNor (None, 703, 272)      1088        merge_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 703, 272)      0           batch_normalization_28[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)               (None, 703, 12)       48972       activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)             (None, 703, 12)       0           conv1d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_21 (Merge)                 (None, 703, 284)      0           average_pooling1d_3[0][0]        \n",
      "                                                                   dropout_22[0][0]                 \n",
      "                                                                   dropout_23[0][0]                 \n",
      "                                                                   dropout_24[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNor (None, 703, 284)      1136        merge_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 703, 284)      0           batch_normalization_29[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)               (None, 703, 12)       51132       activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 703, 12)       0           conv1d_41[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_22 (Merge)                 (None, 703, 296)      0           average_pooling1d_3[0][0]        \n",
      "                                                                   dropout_22[0][0]                 \n",
      "                                                                   dropout_23[0][0]                 \n",
      "                                                                   dropout_24[0][0]                 \n",
      "                                                                   dropout_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNor (None, 703, 296)      1184        merge_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 703, 296)      0           batch_normalization_30[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)               (None, 703, 12)       53292       activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 703, 12)       0           conv1d_42[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_23 (Merge)                 (None, 703, 308)      0           average_pooling1d_3[0][0]        \n",
      "                                                                   dropout_22[0][0]                 \n",
      "                                                                   dropout_23[0][0]                 \n",
      "                                                                   dropout_24[0][0]                 \n",
      "                                                                   dropout_25[0][0]                 \n",
      "                                                                   dropout_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNor (None, 703, 308)      1232        merge_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 703, 308)      0           batch_normalization_31[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)               (None, 703, 12)       55452       activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 703, 12)       0           conv1d_43[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "merge_24 (Merge)                 (None, 703, 320)      0           average_pooling1d_3[0][0]        \n",
      "                                                                   dropout_22[0][0]                 \n",
      "                                                                   dropout_23[0][0]                 \n",
      "                                                                   dropout_24[0][0]                 \n",
      "                                                                   dropout_25[0][0]                 \n",
      "                                                                   dropout_26[0][0]                 \n",
      "                                                                   dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glob (None, 320)           0           merge_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 38)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 358)           0           global_average_pooling1d_1[0][0] \n",
      "                                                                   input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNor (None, 358)           1432        concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 358)           0           batch_normalization_32[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           45952       activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)             (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 448)           0           dropout_28[0][0]                 \n",
      "                                                                   global_average_pooling1d_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNor (None, 448)           1792        concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 448)           0           batch_normalization_33[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            28736       activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (None, 64)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             65          dropout_29[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,400,425\n",
      "Trainable params: 2,389,469\n",
      "Non-trainable params: 10,956\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:99: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso..., name=\"create_dense_net\")`\n"
     ]
    }
   ],
   "source": [
    "model = create_dense_net(depth=depth, nb_dense_block=4, growth_rate=12, nb_filter=32, dropout_rate=0.3,verbose=True)\n",
    "adam = Adam(lr=lr_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='mae', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to  train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/ ...........\n"
     ]
    }
   ],
   "source": [
    "'''model name'''\n",
    "time_str = time.strftime(\"%m%d\")\n",
    "drop = str(math.ceil(dropout_rate/0.1))\n",
    "model_name = time_str + '_D' + drop + '_l' + str(l_2) + '_C10_MCNN_Dense_'+activation+'_L'+str(depth)+'_Meta=' + str(use_meta) +'_idenFC'\n",
    "directory = 'train_' + time_str + '/' + model_name + '/' \n",
    "\n",
    "'''model training'''\n",
    "print('Model will be saved to ', directory, '...........')\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(directory + \"model.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='auto')\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.loss=[]\n",
    "        self.val_loss=[]\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "\n",
    "loss_history = LossHistory()\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 30, mode = 'auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 5, min_lr = 0, cooldown = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# records =  1610\n",
      "Epoch 1/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 23.0811Epoch 00000: val_loss improved from inf to 19.40102, saving model to train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/model.h5\n",
      "200/200 [==============================] - 217s - loss: 23.0287 - val_loss: 19.4010\n",
      "Epoch 2/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.7939Epoch 00001: val_loss improved from 19.40102 to 18.27685, saving model to train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/model.h5\n",
      "200/200 [==============================] - 164s - loss: 12.8009 - val_loss: 18.2768\n",
      "Epoch 3/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.7515Epoch 00002: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 12.7481 - val_loss: 19.8612\n",
      "Epoch 4/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.1956Epoch 00003: val_loss improved from 18.27685 to 14.99115, saving model to train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/model.h5\n",
      "200/200 [==============================] - 165s - loss: 12.1954 - val_loss: 14.9912\n",
      "Epoch 5/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.0346Epoch 00004: val_loss improved from 14.99115 to 12.96885, saving model to train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/model.h5\n",
      "200/200 [==============================] - 165s - loss: 12.0309 - val_loss: 12.9688\n",
      "Epoch 6/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 12.0174Epoch 00005: val_loss improved from 12.96885 to 12.75260, saving model to train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/model.h5\n",
      "200/200 [==============================] - 165s - loss: 12.0201 - val_loss: 12.7526\n",
      "Epoch 7/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.9267Epoch 00006: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.9140 - val_loss: 14.4457\n",
      "Epoch 8/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.5900Epoch 00007: val_loss did not improve\n",
      "200/200 [==============================] - 165s - loss: 11.5929 - val_loss: 13.3992\n",
      "Epoch 9/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.9124Epoch 00008: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.9075 - val_loss: 13.8148\n",
      "Epoch 10/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.6827Epoch 00009: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.6736 - val_loss: 14.3834\n",
      "Epoch 11/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.5747Epoch 00010: val_loss improved from 12.75260 to 12.72417, saving model to train_0924/0924_D4_l1e-06_C10_MCNN_Dense_relu_L29_Meta=True_idenFC/model.h5\n",
      "200/200 [==============================] - 165s - loss: 11.5808 - val_loss: 12.7242\n",
      "Epoch 12/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.3946Epoch 00011: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.3893 - val_loss: 14.8316\n",
      "Epoch 13/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.3708Epoch 00012: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.3731 - val_loss: 14.7766\n",
      "Epoch 14/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.4217Epoch 00013: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.4207 - val_loss: 15.5743\n",
      "Epoch 15/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.1664Epoch 00014: val_loss did not improve\n",
      "200/200 [==============================] - 165s - loss: 11.1564 - val_loss: 14.4219\n",
      "Epoch 16/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 11.2067Epoch 00015: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 11.2161 - val_loss: 16.1688\n",
      "Epoch 17/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.9519Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.00020000000949949026.\n",
      "200/200 [==============================] - 165s - loss: 10.9532 - val_loss: 15.2879\n",
      "Epoch 18/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.9006Epoch 00017: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.9033 - val_loss: 13.5769\n",
      "Epoch 19/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.7207Epoch 00018: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.7226 - val_loss: 14.9646\n",
      "Epoch 20/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.5321Epoch 00019: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.5308 - val_loss: 15.4228\n",
      "Epoch 21/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.3678Epoch 00020: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.3628 - val_loss: 14.2225\n",
      "Epoch 22/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.1712Epoch 00021: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.1770 - val_loss: 15.8190\n",
      "Epoch 23/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.2451Epoch 00022: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.2475 - val_loss: 15.8059\n",
      "Epoch 24/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.2830Epoch 00023: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 10.2794 - val_loss: 15.7161\n",
      "Epoch 25/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.9442Epoch 00024: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.9443 - val_loss: 15.9348\n",
      "Epoch 26/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.9749Epoch 00025: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.9890 - val_loss: 17.2836\n",
      "Epoch 27/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 10.1257Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 4.0000001899898055e-05.\n",
      "200/200 [==============================] - 164s - loss: 10.1230 - val_loss: 16.4070\n",
      "Epoch 28/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.9276Epoch 00027: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.9311 - val_loss: 15.8248\n",
      "Epoch 29/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.8997Epoch 00028: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.8911 - val_loss: 16.4942\n",
      "Epoch 30/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.8195Epoch 00029: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.8180 - val_loss: 16.7027\n",
      "Epoch 31/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.8262Epoch 00030: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.8235 - val_loss: 16.4545\n",
      "Epoch 32/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.7474Epoch 00031: val_loss did not improve\n",
      "200/200 [==============================] - 164s - loss: 9.7500 - val_loss: 16.7121\n",
      "Epoch 33/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.6874Epoch 00032: val_loss did not improve\n",
      "200/200 [==============================] - 163s - loss: 9.6909 - val_loss: 16.9788\n",
      "Epoch 34/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.7526Epoch 00033: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.7528 - val_loss: 17.0791\n",
      "Epoch 35/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.7996Epoch 00034: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.8117 - val_loss: 16.8844\n",
      "Epoch 36/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.8674Epoch 00035: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.8753 - val_loss: 17.4661\n",
      "Epoch 37/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.7821Epoch 00036: val_loss did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 8.000000525498762e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 162s - loss: 9.7769 - val_loss: 17.0344\n",
      "Epoch 38/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.5729Epoch 00037: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.5779 - val_loss: 17.2038\n",
      "Epoch 39/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.6501Epoch 00038: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.6492 - val_loss: 17.1398\n",
      "Epoch 40/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.5824Epoch 00039: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.5801 - val_loss: 17.1185\n",
      "Epoch 41/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.6565Epoch 00040: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.6473 - val_loss: 17.1932\n",
      "Epoch 42/50\n",
      "199/200 [============================>.] - ETA: 0s - loss: 9.7846Epoch 00041: val_loss did not improve\n",
      "200/200 [==============================] - 162s - loss: 9.7810 - val_loss: 17.1189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3afcf4748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Val_X5, Val_X10, Val_X25, Val_X50, Val_M,Val_Y = validation_generation(test_file,test_lab)\n",
    "model.fit_generator(generator=train_generator(batch_size),\n",
    "                validation_data=([Val_X5, Val_X10, Val_X25,Val_X50,Val_M],Val_Y),\n",
    "                steps_per_epoch = n_batch_per_epoch,\n",
    "                epochs=epochs,\n",
    "                callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:1242: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "'''loss history fig'''\n",
    "matplotlib.use('Agg')\n",
    "sk = 0\n",
    "plt.figure(0)\n",
    "plt.plot(range(len(loss_history.loss[sk:])),loss_history.loss[sk:],label='train_loss')\n",
    "plt.plot(range(len(loss_history.val_loss[sk:])),loss_history.val_loss[sk:],label='val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0, max(loss_history.val_loss+loss_history.loss)])\n",
    "plt.savefig(directory + \"loss_history.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "'''plot train/test mae/cor fig'''\n",
    "tmp = [model_name]\n",
    "\n",
    "# recording\n",
    "tmp.append(loss_history.loss[loss_history.val_loss.index(min(loss_history.val_loss))])\n",
    "tmp.append(min(loss_history.val_loss))\n",
    "tmp.append(loss_history.loss[-1])\n",
    "tmp.append(loss_history.val_loss[-1])\n",
    "\n",
    "model_test = load_model(directory + \"model.h5\")\n",
    "\n",
    "# plot training\n",
    "if use_meta:\n",
    "    train_X1, train_X2, train_X3, train_X4, train_M,train_Y1 = one_train_set()\n",
    "    Y_pred = model_test.predict([train_X1, train_X2, train_X3, train_X4,train_M])\n",
    "else:\n",
    "    train_X1, train_X2, train_X3, train_X4,train_Y1 = one_train_set()\n",
    "    Y_pred = model_test.predict([train_X1, train_X2, train_X3, train_X4])\n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "cor = pearsonr(train_Y1,Y_pred)[0]\n",
    "mae = mean_absolute_error(train_Y1,Y_pred)\n",
    "\n",
    "# recording\n",
    "tmp.append(mae)\n",
    "tmp.append(cor)\n",
    "\n",
    "\n",
    "f = pd.DataFrame({'y_true': train_Y1,'y_pred': Y_pred})\n",
    "f.to_csv(directory + \"pred_train.csv\")\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.scatter(train_Y1, Y_pred, alpha = .15, s = 20)\n",
    "plt.xlabel('True_Y')\n",
    "plt.ylabel('Pred_Y')\n",
    "plt.title(\"Training data \\n\" + \"MAE = %4f; Cor = %4f; #samples = %d\" % (mae,cor, len(train_Y1)))\n",
    "plt.savefig(directory+\"plot_scatter_train.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "# plot testing\n",
    "if use_meta:\n",
    "    Y_pred = model_test.predict([Val_X5, Val_X10, Val_X25, Val_X50, Val_M])\n",
    "else:\n",
    "    Y_pred = model_test.predict([Val_X5, Val_X10, Val_X25, Val_X50])\n",
    "    \n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "f = pd.DataFrame({'y_true': Val_Y,'y_pred': Y_pred})\n",
    "f.to_csv(directory + \"pred_test.csv\")\n",
    "\n",
    "cor = pearsonr(Val_Y,Y_pred)[0]\n",
    "mae = mean_absolute_error(Val_Y,Y_pred)\n",
    "\n",
    "# recording\n",
    "tmp.append(mae)\n",
    "tmp.append(cor)\n",
    "\n",
    "'''scatter plot for test data'''\n",
    "plt.figure(0)\n",
    "plt.scatter(Val_Y, Y_pred, alpha = .15, s = 20)\n",
    "plt.xlabel('True_Y')\n",
    "plt.ylabel('Pred_Y')\n",
    "plt.title(\"Testing data \\n\" + \"MAE = %4f; Cor = %4f; #samples = %d\" % (mae, cor, len(Val_Y)))\n",
    "plt.savefig(directory + \"plot_scatter_test.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "'''save model detail'''\n",
    "f = pd.DataFrame([tmp], columns = ['model_name', 'train_loss', 'val_loss', 'final_train_loss', 'final_val_loss', 'train_mae', 'train_cor', 'test_mae', 'test_cor'])\n",
    "f.to_csv(directory + 'model_detail.csv')\n",
    "\n",
    "'''save test ID'''\n",
    "f2 = pd.DataFrame([ID for ID in test_file], columns = ['test_ID'])\n",
    "f2.to_csv(directory + 'test_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%javascript\\nJupyter.notebook.session.delete();\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%javascript\n",
    "Jupyter.notebook.session.delete();\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
