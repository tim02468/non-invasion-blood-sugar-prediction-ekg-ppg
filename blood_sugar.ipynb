{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=6\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,Conv1D, Dense, MaxPool1D, Activation, AvgPool1D,GlobalAveragePooling1D\n",
    "from keras.layers import Flatten, Add, Concatenate, Dropout, BatchNormalization, LSTM,GlobalMaxPooling1D\n",
    "from keras import regularizers, initializers\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats.stats import pearsonr\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from data_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_CNN = False\n",
    "activation = 'relu' # selu\n",
    "use_meta = True\n",
    "model_select = 'andrew' # andrew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''read signal'''\n",
    "file_path = '/data/put_data/timliu/BG/1006_data/'\n",
    "if multi_CNN:\n",
    "    with open(file_path+'dict_X_C10_S5_50_5sec.pickle', 'rb') as handle:\n",
    "        X50 = pickle.load(handle)\n",
    "    with open(file_path+'dict_X_C10_S5_100_5sec.pickle', 'rb') as handle: # 100\n",
    "        X5 = pickle.load(handle) \n",
    "    with open(file_path+'dict_X_C10_S5_200_5sec.pickle', 'rb') as handle: \n",
    "        X10 = pickle.load(handle) \n",
    "    with open(file_path+'dict_X_C10_S5_25_5sec.pickle', 'rb') as handle: \n",
    "        X25 = pickle.load(handle) \n",
    "else:\n",
    "    if model_select == 'LSTM':\n",
    "        with open(file_path+'dict_X_C10_40_5sec.pickle', 'rb') as handle:\n",
    "            X = pickle.load(handle)\n",
    "    else:\n",
    "        with open(file_path+'dict_X_C10_200_1020_final_5sec.pickle', 'rb') as handle:\n",
    "            X = pickle.load(handle)\n",
    "'''read meta'''\n",
    "with open(file_path+'dict_meta_1020.pickle', 'rb') as handle:\n",
    "    meta = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, key in enumerate(X):\n",
    "    for index2, key2 in enumerate(X[key]):\n",
    "        X[key][key2] = X[key][key2][:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2914, 19)\n"
     ]
    }
   ],
   "source": [
    "d = pd.read_csv(\"/data/put_data/timliu/BG/new_IRB_summary_1457_1020.csv\",dtype=\"str\")\n",
    "d = d.iloc[:, 2:]\n",
    "for index, row in d.iterrows():\n",
    "    if 'm' in row['ID']:\n",
    "        d.iloc[index, 0] = d.iloc[index, 0] + '_m'\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d = pd.DataFrame([row for index, row in d.iterrows() if 'm' not in row['ID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Person No</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>SYS</th>\n",
       "      <th>DIA</th>\n",
       "      <th>HR</th>\n",
       "      <th>G</th>\n",
       "      <th>PWV</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP_drug</th>\n",
       "      <th>DM</th>\n",
       "      <th>DM_drug</th>\n",
       "      <th>O_drug</th>\n",
       "      <th>W_cir</th>\n",
       "      <th>weight</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>143</td>\n",
       "      <td>104</td>\n",
       "      <td>73</td>\n",
       "      <td>106</td>\n",
       "      <td>6.5</td>\n",
       "      <td>25.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>62.7</td>\n",
       "      <td>003_1</td>\n",
       "      <td>20170302</td>\n",
       "      <td>0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>144</td>\n",
       "      <td>95</td>\n",
       "      <td>73</td>\n",
       "      <td>95</td>\n",
       "      <td>6.5</td>\n",
       "      <td>25.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>62.7</td>\n",
       "      <td>003_2</td>\n",
       "      <td>20170302</td>\n",
       "      <td>0939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>133</td>\n",
       "      <td>92</td>\n",
       "      <td>80</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>52.6</td>\n",
       "      <td>004_1</td>\n",
       "      <td>20170303</td>\n",
       "      <td>0817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>117</td>\n",
       "      <td>85</td>\n",
       "      <td>75</td>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>23.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>52.6</td>\n",
       "      <td>004_2</td>\n",
       "      <td>20170303</td>\n",
       "      <td>0827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>167.5</td>\n",
       "      <td>152</td>\n",
       "      <td>83</td>\n",
       "      <td>106</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>68.5</td>\n",
       "      <td>005_1</td>\n",
       "      <td>20170303</td>\n",
       "      <td>0837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 Person No Age Gender Height  SYS  DIA   HR    G  PWV   BMI  \\\n",
       "0          1         3  53      0    157  143  104   73  106  6.5  25.4   \n",
       "1          2         3  53      0    157  144   95   73   95  6.5  25.4   \n",
       "2          3         4  49      0    150  133   92   80   94    7  23.4   \n",
       "3          4         4  49      0    150  117   85   75   99    7  23.4   \n",
       "4          5         5  55      1  167.5  152   83  106   79    7  24.4   \n",
       "\n",
       "  BP_drug DM DM_drug O_drug W_cir weight     ID      Date  Time  \n",
       "0       0  0       0      1    83   62.7  003_1  20170302  0935  \n",
       "1       0  0       0      1    83   62.7  003_2  20170302  0939  \n",
       "2       0  0       0      1    80   52.6  004_1  20170303  0817  \n",
       "3       0  0       0      1    80   52.6  004_2  20170303  0827  \n",
       "4       0  0       0      0    91   68.5  005_1  20170303  0837  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = pd.read_csv(\"/data/put_data/timliu/BG/new_IRB_summary_1119_1006.csv\",dtype=\"str\")\n",
    "d2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2 = pd.read_csv(\"/data/put_data/timliu/BG/1020_new_test.csv\",dtype=\"str\")\n",
    "for index, row in d2.iterrows():\n",
    "    if int(row['Person No']) <= 250:\n",
    "        d2.iloc[index, 1] = d2.iloc[index, 1] + '_m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of training:  2330  and counts of testing:  584\n"
     ]
    }
   ],
   "source": [
    "# data split\n",
    "split_ratio = 0.8\n",
    "\n",
    "'''split training and testing data set'''\n",
    "D_id = list(set([row['Person No'] for index, row in d.iterrows() if float(row['G']) > 200]))\n",
    "np.random.shuffle(D_id)\n",
    "# ND_id = list(set([row['Person No'] for index, row in d.iterrows() if float(row['G']) >= 0]))\n",
    "# np.random.shuffle(ND_id)\n",
    "ND_id = list(set([row['Person No'] for index, row in d.iterrows() if row['Person No'] not in D_id]))\n",
    "np.random.shuffle(ND_id)\n",
    "# ND_id = ND_id[:int(len(ND_id)*0.4)]\n",
    "\n",
    "train_id = np.hstack((ND_id[:int(len(ND_id) * split_ratio)], D_id[:int(len(D_id) * split_ratio)]))\n",
    "# train_id = np.hstack((ND_id[:int(len(ND_id) * split_ratio)]))\n",
    "train_id\n",
    "\n",
    "test_id = np.hstack((ND_id[int(len(ND_id) * split_ratio):], D_id[int(len(D_id) * split_ratio):]))\n",
    "# test_id = np.hstack((ND_id[int(len(ND_id) * split_ratio):]))\n",
    "test_id\n",
    "\n",
    "test_ind = [index for index, row in d.iterrows() if np.any(test_id == row['Person No'])] \n",
    "np.random.shuffle(test_ind)\n",
    "train_ind = [index for index, row in d.iterrows() if np.any(train_id == row['Person No'])]   \n",
    "np.random.shuffle(train_ind)\n",
    "\n",
    "train_file = np.array(d['ID'][train_ind])\n",
    "test_file = np.array(d['ID'][test_ind])\n",
    "lab = np.array(d['G']).astype('float32')\n",
    "\n",
    "# train_lab = np.array(lab[train_ind])\n",
    "# test_lab  = np.array(lab[test_ind])\n",
    "\n",
    "train_lab = {f:d[d['ID'] == f]['G'].astype('float32').values for f in train_file}\n",
    "test_lab = {f:d[d['ID'] == f]['G'].astype('float32').values for f in test_file}\n",
    "full_lab = {f:d[d['ID'] == f]['G'].astype('float32').values for f in np.append(train_file, test_file)}\n",
    "print('counts of training: ', train_file.shape[0], ' and counts of testing: ', test_file.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameter\n",
    "l_2 = 0\n",
    "dropout_rate = 0\n",
    "lr_rate = 1e-6\n",
    "\n",
    "# Embedding\n",
    "n_sample = 6000\n",
    "n_meta = len(meta['003_1'])\n",
    "n_channel = 5\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 16\n",
    "dense_layers = [100,70,1]\n",
    "local_filters = 64\n",
    "local_kernel_size = 3\n",
    "local_pool_size = 2\n",
    "filters = 64\n",
    "strides = 1\n",
    "layers = 7\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "n_batch_per_epoch = int(train_file.shape[0]/batch_size)\n",
    "epochs = 300\n",
    "kernel_initializer = 'he_uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM + CNN\n",
    "def model_lstm():\n",
    "    model_input = Input((n_sample, n_channel)) \n",
    "    x = Conv1D(filters = 64, kernel_size = 16, strides = 2, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(model_input)\n",
    "    x = Conv1D(filters = 64, kernel_size = 16, strides = 2, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(model_input)\n",
    "    x = Conv1D(filters = 64, kernel_size = 16, strides = 2, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(model_input)\n",
    "    \n",
    "    l = LSTM(units =100, return_sequences=True)(model_input)\n",
    "    l = Dropout(dropout_rate)(l)    \n",
    "    l = LSTM(units = 50, return_sequences=False)(l) \n",
    "    l = Dropout(dropout_rate)(l)    \n",
    "    l = Dense(64, activation=\"relu\")(l)        \n",
    "    #c = Reshape((len(col_ids), pre_num +1, 1))(model_input)     \n",
    "    x = Dense(64, activation=\"relu\")(l)    \n",
    "    x = Dropout(dropout_rate)(x)  \n",
    "    x = Dense(32, activation=\"relu\")(x) \n",
    "    x = Dropout(dropout_rate)(x)   \n",
    "    output = Dense(1, activation=\"linear\")(x)     \n",
    "    model = Model(inputs = [model_input], outputs = [output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResidualBlock(filters,kernel_size,strides,pool_size,inputs):\n",
    "    if activation == 'selu':\n",
    "        new_input = MaxPool1D(pool_size=pool_size, padding = 'same', strides = strides)(inputs)\n",
    "        new_inp_2 = Activation(activation='selu')(inputs)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = Activation(activation='selu')(new_inp_2)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = Add()([new_inp_2, new_input])\n",
    "        return new_inp_2\n",
    "    else:\n",
    "        new_input = MaxPool1D(pool_size=pool_size, padding = 'same', strides = strides)(inputs)\n",
    "        new_inp_2 = BatchNormalization()(inputs)\n",
    "        new_inp_2 = Activation(activation=activation)(new_inp_2)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = BatchNormalization()(new_inp_2)\n",
    "        new_inp_2 = Activation(activation=activation)(new_inp_2)\n",
    "        new_inp_2 = Dropout(dropout_rate)(new_inp_2)\n",
    "        new_inp_2 = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp_2)\n",
    "        new_inp_2 = Add()([new_inp_2, new_input])\n",
    "        return new_inp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_cnn(filters,kernel_size,strides,pool_size,inputs):\n",
    "    if activation == 'selu':\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inputs)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = MaxPool1D(pool_size = local_pool_size, padding = 'same')(inp)\n",
    "        inp = Activation(activation='selu')(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "    else:\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inputs)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = MaxPool1D(pool_size = local_pool_size, padding = 'same')(inp)\n",
    "        inp = BatchNormalization()(inp)\n",
    "        inp = Activation(activation=activation)(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "        inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',kernel_initializer = kernel_initializer)(inp)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    global strides, filters\n",
    "    print('Use multi CNN: ', multi_CNN, ' activation function: ', activation, '\\n')\n",
    "    print('Start building model.....')\n",
    "    if use_meta:\n",
    "        meta_inp = Input(shape=(n_meta,))\n",
    "    if multi_CNN:\n",
    "        sig_inp5 = Input(shape = (3000, 10))\n",
    "        inp5 = local_cnn(local_filters,local_kernel_size,strides,local_pool_size,sig_inp5)\n",
    "\n",
    "        sig_inp10 = Input(shape = (6000, 10))\n",
    "        inp10 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp10)\n",
    "\n",
    "        sig_inp25 = Input(shape = (750, 10))\n",
    "        inp25 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp25)\n",
    "\n",
    "        sig_inp50 = Input(shape = (1500, 10))\n",
    "        inp50 = local_cnn(local_filters, local_kernel_size, strides, local_pool_size, sig_inp50)\n",
    "        if activation == 'selu':\n",
    "            inp_con = keras.layers.concatenate([inp5, inp10, inp25, inp50], axis = 1)\n",
    "            inp_max = MaxPool1D(pool_size=2, padding = 'same')(inp_con)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(inp_con)\n",
    "            l1 = Activation(activation='selu')(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(l1)\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "\n",
    "            new_inp = Flatten()(new_inp)\n",
    "            new_inp = Dense(3072,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "            new_inp = Activation(activation='selu')(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(512,activation='selu', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(128,activation='selu', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        else:\n",
    "            inp_con = keras.layers.concatenate([inp5, inp10, inp25, inp50], axis = 1)\n",
    "            inp_max = MaxPool1D(pool_size=2, padding = 'same')(inp_con)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(inp_con)\n",
    "            l1 = BatchNormalization()(l1)\n",
    "            l1 = Activation(activation=activation)(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\",kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(l1)\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "            new_inp = Flatten()(new_inp)\n",
    "#             new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "            new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dense(192,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "#             new_inp = Dense(128,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "#             new_inp = Dense(128,kernel_regularizer=regularizers.l2(l_2),kernel_initializer = kernel_initializer)(new_inp)\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(128,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(128,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            #new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        if use_meta:\n",
    "            model = Model(inputs=[sig_inp5, sig_inp10, sig_inp25, sig_inp50,meta_inp],outputs=[out])\n",
    "        else:\n",
    "            model = Model(inputs=[sig_inp5, sig_inp10, sig_inp25, sig_inp50],outputs=[out])\n",
    "    else:\n",
    "        sig_inp =  Input(shape=(n_sample, n_channel))        \n",
    "        if activation == 'selu':\n",
    "            inp = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(sig_inp)\n",
    "            inp = Activation(activation='selu')(inp)\n",
    "            inp_max = MaxPool1D(pool_size=2)(inp)\n",
    "\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\")(inp)\n",
    "            l1 = Activation(activation='selu')(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(l1)\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "\n",
    "            new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "            new_inp = Activation(activation='selu')(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            new_inp = Dense(256,activation='selu', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        else:\n",
    "            inp = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(sig_inp)\n",
    "            inp = BatchNormalization()(inp)\n",
    "            inp = Activation(activation=activation)(inp)\n",
    "            inp_max = MaxPool1D(pool_size=2)(inp)\n",
    "\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=2,padding=\"same\")(inp)\n",
    "            l1 = BatchNormalization()(l1)\n",
    "            l1 = Activation(activation=activation)(l1)\n",
    "            l1 = Dropout(dropout_rate)(l1)\n",
    "            l1 = Conv1D(filters=filters,kernel_size=kernel_size,strides=strides,padding=\"same\")(l1)\n",
    "\n",
    "            new_inp = Add()([l1,inp_max])\n",
    "\n",
    "            for i in range(layers):\n",
    "            # every alternate residual block subsample its input by a factor of 2\n",
    "                if i % 2 == 1:\n",
    "                    pool_size = 2\n",
    "                    strides = 2\n",
    "                else:\n",
    "                    pool_size = 1\n",
    "                    strides = 1\n",
    "                # incremented filters    \n",
    "                if i % 4 == 3:\n",
    "                    filters = 64*int(i//4 + 2)\n",
    "                    new_inp = Conv1D(filters = filters, kernel_size = kernel_size, strides = 1, padding = 'same',kernel_initializer = kernel_initializer, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "                new_inp = ResidualBlock(filters,kernel_size,strides,pool_size,new_inp)\n",
    "            \n",
    "            new_inp = LSTM(units = 100, activation = 'relu', return_sequences=True)(new_inp)  \n",
    "            new_inp = LSTM(units = 50, activation = 'relu', return_sequences=False)(new_inp)\n",
    "#             new_inp = GlobalAveragePooling1D()(new_inp)\n",
    "#             new_inp = BatchNormalization()(new_inp)\n",
    "#             new_inp = Activation(activation=activation)(new_inp)\n",
    "#             new_inp = Dense(3072,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            if use_meta:\n",
    "                new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            new_inp = Dense(64,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "            new_inp = Dense(64,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "            new_inp = Dropout(dropout_rate)(new_inp)\n",
    "#             new_inp = Dense(256,activation=activation, kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "#             new_inp = Dropout(dropout_rate)(new_inp)\n",
    "#             if use_meta:\n",
    "#                 new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            #new_inp = keras.layers.concatenate([new_inp,meta_inp])\n",
    "            out = Dense(1,activation='linear', kernel_regularizer=regularizers.l2(l_2))(new_inp)\n",
    "        if use_meta:\n",
    "            model = Model(inputs=[sig_inp,meta_inp],outputs=[out])\n",
    "        else:\n",
    "            model = Model(inputs=[sig_inp],outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vgg 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use multi CNN:  False  activation function:  relu \n",
      "\n",
      "Start building model.....\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_20 (InputLayer)            (None, 6000, 5)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_227 (Conv1D)              (None, 6000, 64)      5184        input_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchNo (None, 6000, 64)      256         conv1d_227[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_202 (Activation)      (None, 6000, 64)      0           batch_normalization_202[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_228 (Conv1D)              (None, 3000, 64)      65600       activation_202[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchNo (None, 3000, 64)      256         conv1d_228[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_203 (Activation)      (None, 3000, 64)      0           batch_normalization_203[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_219 (Dropout)            (None, 3000, 64)      0           activation_203[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_229 (Conv1D)              (None, 3000, 64)      65600       dropout_219[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_100 (MaxPooling1D) (None, 3000, 64)      0           activation_202[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_100 (Add)                    (None, 3000, 64)      0           conv1d_229[0][0]                 \n",
      "                                                                   max_pooling1d_100[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchNo (None, 3000, 64)      256         add_100[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_204 (Activation)      (None, 3000, 64)      0           batch_normalization_204[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_220 (Dropout)            (None, 3000, 64)      0           activation_204[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_230 (Conv1D)              (None, 3000, 64)      65600       dropout_220[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchNo (None, 3000, 64)      256         conv1d_230[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_205 (Activation)      (None, 3000, 64)      0           batch_normalization_205[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_221 (Dropout)            (None, 3000, 64)      0           activation_205[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_231 (Conv1D)              (None, 3000, 64)      65600       dropout_221[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_101 (MaxPooling1D) (None, 3000, 64)      0           add_100[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_101 (Add)                    (None, 3000, 64)      0           conv1d_231[0][0]                 \n",
      "                                                                   max_pooling1d_101[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchNo (None, 3000, 64)      256         add_101[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_206 (Activation)      (None, 3000, 64)      0           batch_normalization_206[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_222 (Dropout)            (None, 3000, 64)      0           activation_206[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_232 (Conv1D)              (None, 1500, 64)      65600       dropout_222[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchNo (None, 1500, 64)      256         conv1d_232[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_207 (Activation)      (None, 1500, 64)      0           batch_normalization_207[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_223 (Dropout)            (None, 1500, 64)      0           activation_207[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_233 (Conv1D)              (None, 1500, 64)      65600       dropout_223[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_102 (MaxPooling1D) (None, 1500, 64)      0           add_101[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_102 (Add)                    (None, 1500, 64)      0           conv1d_233[0][0]                 \n",
      "                                                                   max_pooling1d_102[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchNo (None, 1500, 64)      256         add_102[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_208 (Activation)      (None, 1500, 64)      0           batch_normalization_208[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_224 (Dropout)            (None, 1500, 64)      0           activation_208[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_234 (Conv1D)              (None, 1500, 64)      65600       dropout_224[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchNo (None, 1500, 64)      256         conv1d_234[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_209 (Activation)      (None, 1500, 64)      0           batch_normalization_209[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_225 (Dropout)            (None, 1500, 64)      0           activation_209[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_235 (Conv1D)              (None, 1500, 64)      65600       dropout_225[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_103 (MaxPooling1D) (None, 1500, 64)      0           add_102[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_103 (Add)                    (None, 1500, 64)      0           conv1d_235[0][0]                 \n",
      "                                                                   max_pooling1d_103[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_236 (Conv1D)              (None, 1500, 128)     131200      add_103[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchNo (None, 1500, 128)     512         conv1d_236[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_210 (Activation)      (None, 1500, 128)     0           batch_normalization_210[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_226 (Dropout)            (None, 1500, 128)     0           activation_210[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_237 (Conv1D)              (None, 750, 128)      262272      dropout_226[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchNo (None, 750, 128)      512         conv1d_237[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_211 (Activation)      (None, 750, 128)      0           batch_normalization_211[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_227 (Dropout)            (None, 750, 128)      0           activation_211[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_238 (Conv1D)              (None, 750, 128)      262272      dropout_227[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_104 (MaxPooling1D) (None, 750, 128)      0           conv1d_236[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "add_104 (Add)                    (None, 750, 128)      0           conv1d_238[0][0]                 \n",
      "                                                                   max_pooling1d_104[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchNo (None, 750, 128)      512         add_104[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_212 (Activation)      (None, 750, 128)      0           batch_normalization_212[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_228 (Dropout)            (None, 750, 128)      0           activation_212[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_239 (Conv1D)              (None, 750, 128)      262272      dropout_228[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchNo (None, 750, 128)      512         conv1d_239[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_213 (Activation)      (None, 750, 128)      0           batch_normalization_213[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_229 (Dropout)            (None, 750, 128)      0           activation_213[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_240 (Conv1D)              (None, 750, 128)      262272      dropout_229[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_105 (MaxPooling1D) (None, 750, 128)      0           add_104[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_105 (Add)                    (None, 750, 128)      0           conv1d_240[0][0]                 \n",
      "                                                                   max_pooling1d_105[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchNo (None, 750, 128)      512         add_105[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_214 (Activation)      (None, 750, 128)      0           batch_normalization_214[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_230 (Dropout)            (None, 750, 128)      0           activation_214[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_241 (Conv1D)              (None, 375, 128)      262272      dropout_230[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchNo (None, 375, 128)      512         conv1d_241[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_215 (Activation)      (None, 375, 128)      0           batch_normalization_215[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_231 (Dropout)            (None, 375, 128)      0           activation_215[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_242 (Conv1D)              (None, 375, 128)      262272      dropout_231[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_106 (MaxPooling1D) (None, 375, 128)      0           add_105[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_106 (Add)                    (None, 375, 128)      0           conv1d_242[0][0]                 \n",
      "                                                                   max_pooling1d_106[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchNo (None, 375, 128)      512         add_106[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_216 (Activation)      (None, 375, 128)      0           batch_normalization_216[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_232 (Dropout)            (None, 375, 128)      0           activation_216[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_243 (Conv1D)              (None, 375, 128)      262272      dropout_232[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchNo (None, 375, 128)      512         conv1d_243[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_217 (Activation)      (None, 375, 128)      0           batch_normalization_217[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_233 (Dropout)            (None, 375, 128)      0           activation_217[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_244 (Conv1D)              (None, 375, 128)      262272      dropout_233[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_107 (MaxPooling1D) (None, 375, 128)      0           add_106[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "add_107 (Add)                    (None, 375, 128)      0           conv1d_244[0][0]                 \n",
      "                                                                   max_pooling1d_107[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                   (None, 375, 100)      91600       add_107[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                   (None, 50)            30200       lstm_11[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_19 (InputLayer)            (None, 14)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)      (None, 64)            0           lstm_12[0][0]                    \n",
      "                                                                   input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 64)            4160        concatenate_9[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_234 (Dropout)            (None, 64)            0           dense_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 64)            4160        dropout_234[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_235 (Dropout)            (None, 64)            0           dense_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                 (None, 1)             65          dropout_235[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 2,895,689\n",
      "Trainable params: 2,892,617\n",
      "Non-trainable params: 3,072\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if model_select == 'andrew':\n",
    "    model = build_model()\n",
    "elif model_select == 'LSTM':\n",
    "    model = model_lstm()\n",
    "adam = Adam(lr=lr_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay = 0.0)\n",
    "model.compile(loss='mae', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory and Model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be saved to  train_1029/D0_l1e-07_multicnn=False_L33_FLAT_Meta=True_FC128/ ...\n"
     ]
    }
   ],
   "source": [
    "'''model name'''\n",
    "time_str = time.strftime(\"%m%d\")\n",
    "drop = str(math.ceil(dropout_rate/0.1))\n",
    "if model_select == 'andrew':\n",
    "    model_name =  'D' + drop + '_l' + str(l_2) +'_multicnn='+str(multi_CNN)+'_L'+str(layers*2+3)+'_FLAT_Meta=' + str(use_meta) + '_FC128'\n",
    "elif model_select == 'LSTM':\n",
    "    model_name = 'D' + drop + '_l' + str(l_2) + '_LSTM'  \n",
    "directory = 'train_' + time_str + '/' + model_name + '/' \n",
    "\n",
    "'''model training'''\n",
    "print('Model will be saved to ', directory, '...')\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(directory + \"model.h5\",\n",
    "                    monitor='val_loss',\n",
    "                    verbose=1,\n",
    "                    save_best_only=True,\n",
    "                    mode='auto')\n",
    "\n",
    "def figure_making(train_loss, val_loss):\n",
    "    sk = 0\n",
    "    plt.figure(0)\n",
    "    plt.plot(range(len(train_loss[sk:])),train_loss[sk:],label='train_loss')\n",
    "    plt.plot(range(len(val_loss[sk:])),val_loss[sk:],label='val_loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.ylim([0, max(train_loss+val_loss)])\n",
    "    plt.savefig(directory + \"loss_history.png\")\n",
    "    plt.close()\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.loss=[]\n",
    "        self.val_loss=[]\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        if len(self.loss) % 30 == 0 and len(self.loss) != 0:\n",
    "            figure_making(self.loss, self.val_loss)\n",
    "\n",
    "loss_history = LossHistory()\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience = 50, mode = 'auto')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience = 10, min_lr = 0, cooldown = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# records =  2200\n",
      "Epoch 1/300\n",
      " 8/77 [==>...........................] - ETA: 95s - loss: 107.8631 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1a27b4127baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batch_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                             callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mVal_X50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVal_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generation_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_lab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if model_select == 'andrew':\n",
    "    if multi_CNN:\n",
    "        if use_meta:\n",
    "            Val_X5, Val_X10, Val_X25, Val_X50, Val_M,Val_Y = validation_generation(test_file,test_lab,multi_CNN,use_meta,X5,X10,X25,X50,meta)\n",
    "            model.fit_generator(generator=train_generator(train_file,batch_size,multi_CNN,use_meta,X5,X10,X25,X50,meta,train_lab),\n",
    "                            validation_data=([Val_X5, Val_X10, Val_X25,Val_X50,Val_M],Val_Y),\n",
    "                            steps_per_epoch = n_batch_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "        else:\n",
    "            Val_X5, Val_X10, Val_X25, Val_X50,Val_Y = validation_generation(test_file,test_lab,multi_CNN,use_meta,X5,X10,X25,X50,meta)\n",
    "            model.fit_generator(generator=train_generator(train_file,batch_size,multi_CNN,use_meta,X5,X10,X25,X50,meta,train_lab),\n",
    "                                    validation_data=([Val_X5, Val_X10, Val_X25,Val_X50],Val_Y),\n",
    "                                    steps_per_epoch = n_batch_per_epoch,\n",
    "                                    epochs=epochs,\n",
    "                                    callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "    else:\n",
    "        if use_meta:\n",
    "            Val_X, Val_M,Val_Y = validation_generation_single(test_file,test_lab,X,meta,use_meta)\n",
    "            model.fit_generator(generator=train_generator_single(train_file, batch_size, X, meta, train_lab, use_meta),\n",
    "                            validation_data=([Val_X,Val_M],Val_Y),\n",
    "                            steps_per_epoch = n_batch_per_epoch,\n",
    "                            epochs=epochs,\n",
    "                            callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "        else:\n",
    "            Val_X50,Val_Y = validation_generation_single(test_file,test_lab,X,meta,use_meta)\n",
    "            model.fit_generator(generator=train_generator_single(train_file, batch_size, X, meta, train_lab, use_meta),\n",
    "                                validation_data=([Val_X50],Val_Y),\n",
    "                                steps_per_epoch = n_batch_per_epoch,\n",
    "                                epochs=epochs,\n",
    "                                callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])\n",
    "        \n",
    "elif model_select == 'LSTM':\n",
    "    Val_X50,Val_Y = validation_generation_single(test_file,test_lab,X,meta,use_meta)\n",
    "    model.fit_generator(generator=train_generator_single(train_file, batch_size, X, meta, train_lab, use_meta),\n",
    "                        validation_data=([Val_X50],Val_Y),\n",
    "                        steps_per_epoch = n_batch_per_epoch,\n",
    "                        epochs=epochs,\n",
    "                        callbacks = [loss_history,checkpoint,earlystopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting and Saving DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''loss history fig'''\n",
    "matplotlib.use('Agg')\n",
    "sk = 0\n",
    "plt.figure(0)\n",
    "plt.plot(range(len(loss_history.loss[sk:])),loss_history.loss[sk:],label='train_loss')\n",
    "plt.plot(range(len(loss_history.val_loss[sk:])),loss_history.val_loss[sk:],label='val_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim([0, max(loss_history.val_loss+loss_history.loss)])\n",
    "plt.savefig(directory + \"loss_history.png\", dpi = 300)\n",
    "plt.close()\n",
    "\n",
    "'''plot train/test mae/cor fig'''\n",
    "tmp = [model_name]\n",
    "\n",
    "# recording\n",
    "tmp.append(loss_history.loss[loss_history.val_loss.index(min(loss_history.val_loss))])\n",
    "tmp.append(min(loss_history.val_loss))\n",
    "tmp.append(loss_history.loss[-1])\n",
    "tmp.append(loss_history.val_loss[-1])\n",
    "\n",
    "model_test = load_model(directory + \"model.h5\")\n",
    "\n",
    "# plot training\n",
    "if use_meta and multi_CNN:\n",
    "    train_X1, train_X2, train_X3, train_X4, train_M,train_Y1 = one_train_set(multi_CNN,use_meta,X5,X10,X25,X50,meta,train_lab,train_file=train_file)\n",
    "    Y_pred = model_test.predict([train_X1, train_X2, train_X3, train_X4,train_M])\n",
    "else:\n",
    "    train_X1, train_meta, train_Y1 = one_train_set_single(X,meta,train_lab,train_file,use_meta,size = Val_Y.shape[0])\n",
    "#     train_X1, train_Y1 = one_train_set_single(X,meta,train_lab,train_file,use_meta,size = Val_Y.shape[0])\n",
    "    Y_pred = model_test.predict([train_X1, train_meta])\n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "cor = pearsonr(train_Y1,Y_pred)[0]\n",
    "mae = mean_absolute_error(train_Y1,Y_pred)\n",
    "\n",
    "# recording\n",
    "tmp.append(mae)\n",
    "tmp.append(cor)\n",
    "\n",
    "\n",
    "f = pd.DataFrame({'y_true': train_Y1,'y_pred': Y_pred})\n",
    "f.to_csv(directory + \"pred_train.csv\")\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.scatter(train_Y1, Y_pred, alpha = .15, s = 20)\n",
    "plt.xlabel('True_Y')\n",
    "plt.ylabel('Pred_Y')\n",
    "plt.title(\"Training data \\n\" + \"MAE = %4f; Cor = %4f; #samples = %d\" % (mae,cor, len(train_Y1)))\n",
    "plt.savefig(directory+\"plot_scatter_train.png\")\n",
    "plt.close()\n",
    "\n",
    "# plot testing\n",
    "if use_meta and multi_CNN:\n",
    "    Y_pred = model_test.predict([Val_X5, Val_X10, Val_X25, Val_X50, Val_M])\n",
    "else:\n",
    "    if use_meta:\n",
    "        Y_pred = model_test.predict([Val_X, Val_M])\n",
    "    else:\n",
    "        Y_pred = model_test.predict([Val_X50])\n",
    "    \n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "f = pd.DataFrame({'y_true': Val_Y,'y_pred': Y_pred})\n",
    "f.to_csv(directory + \"pred_test.csv\")\n",
    "\n",
    "cor = pearsonr(Val_Y,Y_pred)[0]\n",
    "mae = mean_absolute_error(Val_Y,Y_pred)\n",
    "\n",
    "# recording\n",
    "tmp.append(mae)\n",
    "tmp.append(cor)\n",
    "\n",
    "'''scatter plot for test data'''\n",
    "plt.figure(0)\n",
    "plt.scatter(Val_Y, Y_pred, alpha = .15, s = 20)\n",
    "plt.xlabel('True_Y')\n",
    "plt.ylabel('Pred_Y')\n",
    "plt.title(\"Testing data \\n\" + \"MAE = %4f; Cor = %4f; #samples = %d\" % (mae, cor, len(Val_Y)))\n",
    "plt.savefig(directory + \"plot_scatter_test.png\")\n",
    "plt.close()\n",
    "\n",
    "'''save model detail'''\n",
    "f = pd.DataFrame([tmp], columns = ['model_name', 'train_loss', 'val_loss', 'final_train_loss', 'final_val_loss', 'train_mae', 'train_cor', 'test_mae', 'test_cor'])\n",
    "f.to_csv(directory + 'model_detail.csv')\n",
    "\n",
    "'''save test ID'''\n",
    "f2 = pd.DataFrame([ID for ID in test_file], columns = ['test_ID'])\n",
    "f2.to_csv(directory + 'test_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = load_model('train_1002/1002_D0_l0_C10_MCNN_relu_L23_GVP_Meta=True_kernal=32/model.h5')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_file = np.asarray([row['ID'] for index, row in d.iterrows() if row['Person No'] in list(d2['Person No'])])\n",
    "# test_lab = {f:d[d['ID'] == f]['G'].astype('float32').values for f in test_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def validation_generation(test_file,test_lab,multi_CNN,use_meta,X5,X10,X25,X50,meta):\n",
    "#     X5_ = []\n",
    "#     X10_ = []\n",
    "#     X25_ = []\n",
    "#     X50_ = []\n",
    "#     M_ = []\n",
    "#     Y_ = []\n",
    "#     for i in range(len(test_file)):\n",
    "#         x = X5[test_file[i]] \n",
    "#         x10 = X10[test_file[i]]\n",
    "#         x25 = X25[test_file[i]]\n",
    "#         x50 = X50[test_file[i]]\n",
    "#         x2 = meta[test_file[i]]\n",
    "#         j = random.randint(0,4)\n",
    "#         X5_.append(x[j])\n",
    "#         X10_.append(x10[j])\n",
    "#         X25_.append(x25[j])\n",
    "#         X50_.append(x50[j])\n",
    "#         M_.append(x2)\n",
    "#         # Y_.append(test_lab[test_file[i]][0])\n",
    "#         Y_.append(np.array(test_lab[test_file[i]])[0])\n",
    "#     X5_ = np.asarray(X5_)\n",
    "#     X10_ = np.asarray(X10_)\n",
    "#     X25_ = np.asarray(X25_)\n",
    "#     X50_ = np.asarray(X50_)\n",
    "#     M_ = np.asarray(M_)\n",
    "# #     COM_ = [X5_,X10_,X25_,X50_,M_]\n",
    "#     Y_ = np.asarray(Y_)\n",
    "#     print(\"# records = \", len(Y_))\n",
    "#     if use_meta:\n",
    "#         return X5_, X10_,X25_,X50_,M_, Y_ \n",
    "#     else:\n",
    "#         return X5_, X10_,X25_,X50_, Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Val_X5, Val_X10, Val_X25, Val_X50, Val_M,Val_Y = validation_generation(test_file,test_lab,multi_CNN,use_meta,X5,X10,X25,X50,meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Val_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict([Val_X5, Val_X10, Val_X25, Val_X50, Val_M])\n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "f = pd.DataFrame({'y_true': Val_Y,'y_pred': Y_pred})\n",
    "# f.to_csv(directory + \"pred_test.csv\")\n",
    "\n",
    "cor = pearsonr(Val_Y,Y_pred)[0]\n",
    "mae = mean_absolute_error(Val_Y,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_generation_single(test_file,test_lab,X,meta,use_meta):\n",
    "    X_ = []\n",
    "    M_ = []\n",
    "    Y_ = []\n",
    "    ind_ = []\n",
    "    for i in range(len(test_file)):\n",
    "        id_ = test_file[i]\n",
    "        signal_id = random.randint(0,4)\n",
    "        x = X[id_][signal_id]\n",
    "        X_.append(x)\n",
    "        M_.append(meta[id_])\n",
    "        Y_.append(test_lab[id_][0])\n",
    "        ind_.append(test_file[i])\n",
    "    X_ = np.asarray(X_)\n",
    "    M_ = np.asarray(M_)\n",
    "    Y_ = np.asarray(Y_)\n",
    "    print(\"# records = \", len(Y_))\n",
    "    if use_meta:\n",
    "        return X_, M_, Y_, ind_\n",
    "    else:\n",
    "        return X_, Y_\n",
    "\n",
    "Val_X, Val_M,Val_Y, ind = validation_generation_single(test_file,test_lab,X,meta,use_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict([Val_X, Val_M])\n",
    "Y_pred = np.hstack(Y_pred)\n",
    "\n",
    "f = pd.DataFrame({'y_true': Val_Y,'y_pred': Y_pred})\n",
    "# f.to_csv(directory + \"pred_test.csv\")\n",
    "\n",
    "cor = pearsonr(Val_Y,Y_pred)[0]\n",
    "mae = mean_absolute_error(Val_Y,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Output Flatten Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory = 'train_1025/D0_l0_multicnn=False_L23_FLAT_Meta=True/'\n",
    "# model = load_model(directory+'model.h5')\n",
    "# model.layers\n",
    "new_model = Model(inputs=[model.layers[0].input], outputs=model.layers[-12].output)\n",
    "# new_model = Model(inputs=[model.layers[0].input,model.layers[1].input,model.layers[2].input,model.layers[3].input], outputs=model.layers[-11].output)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_file = np.array(d['ID'][:len(d)])\n",
    "\n",
    "def validation_generation_single(test_file,test_lab,X,meta,use_meta):\n",
    "    X_ = []\n",
    "    M_ = []\n",
    "    Y_ = []\n",
    "    ind_ = []\n",
    "    for i in range(len(test_file)):\n",
    "        id_ = test_file[i]\n",
    "        signal_id = random.randint(0,4)\n",
    "        x = X[id_][signal_id]\n",
    "        X_.append(x)\n",
    "        M_.append(meta[id_])\n",
    "        Y_.append(test_lab[id_][0])\n",
    "        ind_.append(test_file[i])\n",
    "    X_ = np.asarray(X_)\n",
    "    M_ = np.asarray(M_)\n",
    "    Y_ = np.asarray(Y_)\n",
    "    print(\"# records = \", len(Y_))\n",
    "    if use_meta:\n",
    "        return X_, M_, Y_, ind_\n",
    "    else:\n",
    "        return X_, Y_\n",
    "Val_X, Val_M, Val_Y, ind_ = validation_generation_single(full_file, full_lab,X, meta, use_meta)\n",
    "\n",
    "t = new_model.predict([Val_X])\n",
    "ind_ = pd.DataFrame(ind_, columns = ['id'])\n",
    "t1 = pd.DataFrame(t, dtype = 'str')\n",
    "t2 = pd.concat([ind_, t1], axis = 1)\n",
    "t2.to_csv(directory+'flatten.csv')\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_file = np.array(d['ID'][:len(d)])\n",
    "# train_lab = np.array(d['G'][:len(d)])\n",
    "# train_lab = train_lab.astype('float32')\n",
    "# Val_X, Val_M,Val_Y = validation_generation_single(test_file,test_lab,X,meta,use_meta)\n",
    "\n",
    "# def validation_generation(test_file, test_lab):\n",
    "#     X5_ = []\n",
    "#     X10_ = []\n",
    "#     X25_ = []\n",
    "#     X50_ = []\n",
    "#     Y_ = []\n",
    "#     ind_ = []\n",
    "#     for i in range(test_file.shape[0]):\n",
    "#         x = X5[test_file[i]] \n",
    "#         x10 = X10[test_file[i]]\n",
    "#         x25 = X25[test_file[i]]\n",
    "#         x50 = X50[test_file[i]]\n",
    "#         X5_.append(x[random.randint(0, 4)])\n",
    "#         X10_.append(x10[random.randint(0, 4)])\n",
    "#         X25_.append(x25[random.randint(0, 4)])\n",
    "#         X50_.append(x50[random.randint(0, 4)])\n",
    "#         Y_.append(test_lab[i])\n",
    "#         ind_.append(test_file[i])\n",
    "#     X5_ = np.asarray(X5_)\n",
    "#     X10_ = np.asarray(X10_)\n",
    "#     X25_ = np.asarray(X25_)\n",
    "#     X50_ = np.asarray(X50_)\n",
    "#     Y_ = np.asarray(Y_)\n",
    "#     return X5_, X10_,X25_,X50_, Y_, ind_\n",
    "\n",
    "# X5_,X10_,X25_,X50_,Y_,ind_ = validation_generation(train_file,train_lab)\n",
    "# t = new_model.predict([X5_,X10_,X25_,X50_])\n",
    "# ind_ = pd.DataFrame(ind_, columns = ['id'])\n",
    "# t1 = pd.DataFrame(t, dtype = 'str')\n",
    "# t2 = pd.concat([ind_, t1], axis = 1)\n",
    "# t2.to_csv(directory+'flatten.csv')\n",
    "# t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%javascript\n",
    "# Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
